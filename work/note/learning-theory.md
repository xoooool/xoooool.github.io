---
title: Learning Theory
layout: note
date: 2010-08-31
update: 2010-08-31
---

So you have the latest and greatest deep kernel hashing transductive projectatron machine algorithm that appears to perform billions of times better than naive Bayes on data sets from UCI, but are you _sure_ it is going to do the right thing once it's released into the wild?

# References

Several of the references below appear in my Encyclopedia of Machine Learning article on generalisation bounds ([preprint](/files/pubs/eml10.pdf)).

## Recommended

* von Luxburg, U. and Schölkopf, B., [Statistical Learning Theory: Models, Concepts, and Results](http://www.kyb.mpg.de/publications/attachments/HHL10_vonLuxburg_5955%5B0%5D.pdf), Handbook of the History of Logic, Vol. 10: Inductive Logic, 2009. ([arXiv:0810.4752v1, 2008](http://arxiv.org/abs/0810.4752)).
  +-- {.annote}
  A good, high-level overview of the key ideas in statistical learning theory. Covers consistency, approximation vs. estimation error, uniform convergence, VC dimension, and the symmetrisation trick in some detail. Touches on luckiness, MDL, rates of convergence, Bayesian methods, and some philosophical issues such as Popper's "dimension of a theory".
  =--

* Mendelson, S., [A Few Notes on Statistical Learning Theory](http://wwwmaths.anu.edu.au/~mendelso/papers/summer02.pdf), In Advanced Lectures on Machine Learning, 2003.
  
* Langford, J. [Tutorial on practical prediction theory for classification](http://jmlr.csail.mit.edu/papers/volume6/langford05a/langford05a.pdf), Journal of Machine Learning Research, 2005.
  
* Boucheron, S., Bousquet, O. and Lugosi, G., [Theory of classification: A survey of some recent advances](http://www.econ.upf.edu/~lugosi/esaimsurvey.pdf), ESAIM: Probability and Statistics, 2005.
  +-- {.annote}
  This is a solid technical companion to (von Luxburg and Schölkopf, 2009) that takes an empirical process theory approach to deriving many of the standard bounds in SLT (VC, Radamacher, PAC-Bayesian). It is not as layman-friendly but for someone with a mathematical background it is a very clear and concise treatment.
  =--
  
* Olivier Bousquet, Stéphane Boucheron and Gábor Lugosi, [Introduction to Statistical Learning Theory](http://www.econ.upf.edu/~lugosi/mlss_slt.pdf), In Advanced Lectures on Machine Learning, 2004.
  
* Ralf Herbrich and Robert C. Williamson, [Learning and Generalization: Theory and Bounds](http://users.cecs.anu.edu.au/~williams/papers/P158.pdf), In the Handbook of Brain Theory and Neural Networks, 2002.



## To Read
