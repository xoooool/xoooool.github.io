---
title: Online Learning
layout: note

date: 2009-07-17
---
Online learning is a modern take on the early perceptron work that uses gradient descent methods to update models as training example are examined sequentially.

In broad categories, various online learning formalisation can be classified as:

- Prediction with Expert Advice
- Online Convex (or linear) Optimisation
- Bandits / Partial Monitoring

References
----------

* J. Abernethy, A. Agarwal, P. L. Bartlett, and A. Rakhlin,  _[A Stochastic View of Optimal Regret through Minimax Duality](http://arxiv.org/abs/0903.5328)_ (2009).

* S. Shalev-Shwartz, O. Shamir, N. Srebro, and K. Sridharan,  _[Stochastic Convex Optimization](http://eprints.pascal-network.org/archive/00005408/01/2009_COLT_ShalShamSreSri.pdf)_ (2009).

* M. Raginsky, R. Marcia, J. Silva, and R. Willett,  _[Sequential probability assignment via online convex programming using exponential families](http://people.ee.duke.edu/~maxim/pubs/raginsky_marcia_silva_willett_ISIT09.pdf)_ (2009).

* P. L. Bartlett, E. Hazan, and A. Rakhlin,  _[Adaptive Online Gradient Descent](http://dblp.uni-trier.de/rec/bibtex/conf/nips/BartlettHR07)_ (2007).

* N. Cesa-Bianchi and G. Lugosi,  _[Prediction, Learning, and Games](http://books.google.com/books?id=zDnRBlazhfYC&dq=Prediction+Learning+and+games&printsec=frontcover&source=bn&hl=en&ei=BQxgSt-eJqCQ6APL85WXCw&sa=X&oi=book_result&ct=result&resnum=4)_ (2006).

* S. Wright, _[Optimization Algorithms in Machine Learning](http://pages.cs.wisc.edu/~swright/nips2010/sjw-nips10.pdf)_, NIPS Tutorial, (2010).

{:.compact }